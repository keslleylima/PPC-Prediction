{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4845cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637e93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PpcEvaluator:\n",
    "    def __init__(self, regressor, dataset):\n",
    "        self.__regressor = regressor\n",
    "        self.__dataset = dataset.get_dataframe()\n",
    "        self.__scaler = MinMaxScaler()\n",
    "        self.__k = 10\n",
    "        self.__error_metrics = [\n",
    "            'r2', 'max_error', 'neg_mean_absolute_error',\n",
    "            'neg_mean_squared_error', 'neg_root_mean_squared_error',\n",
    "            'neg_mean_squared_log_error', 'neg_median_absolute_error'\n",
    "        ]\n",
    "        \n",
    "    def set_scaler(self, scaler):\n",
    "        self.__scaler = scaler\n",
    "    \n",
    "    def __scale(self, dados):\n",
    "        return self.__scaler.fit_transform(dados)\n",
    "\n",
    "    def evaluate(self, metrics, only_scaled=False, display_prediction=False, display_feature_importance=False):\n",
    "        ppc = self.__dataset['PrimePathCoverage'].values\n",
    "        previsores_content = self.__dataset[metrics].values\n",
    "\n",
    "        error_metrics_table = pd.DataFrame(\n",
    "            index=['Mean Abs Error', 'Mean Sqr Error', 'Mean Sqr Log Error', 'Mean Median Error', 'R2 Score'],\n",
    "            columns=['no_scaled', 'scaled']\n",
    "        )\n",
    "\n",
    "        if not only_scaled:\n",
    "            resultados = cross_validate(\n",
    "                    self.__regressor, \n",
    "                    previsores_content, \n",
    "                    ppc, \n",
    "                    cv=self.__k, \n",
    "                    scoring=self.__error_metrics, \n",
    "                    return_estimator=True\n",
    "            )\n",
    "            error_metrics_table['no_scaled']['Mean Abs Error'] = abs(resultados['test_neg_mean_absolute_error'].mean())\n",
    "            error_metrics_table['no_scaled']['Mean Sqr Error'] = abs(resultados['test_neg_mean_squared_error'].mean())\n",
    "            error_metrics_table['no_scaled']['Mean Sqr Log Error'] = abs(resultados['test_neg_mean_squared_log_error'].mean())\n",
    "            error_metrics_table['no_scaled']['Mean Median Error'] = abs(resultados['test_neg_median_absolute_error'].mean())\n",
    "            error_metrics_table['no_scaled']['R2 Score'] = abs(resultados['test_r2'].mean())\n",
    "\n",
    "        resultados_escalonados = cross_validate(\n",
    "                self.__regressor,\n",
    "                self.__scale(previsores_content), \n",
    "                ppc, \n",
    "                cv=self.__k, \n",
    "                scoring=self.__error_metrics, \n",
    "                return_estimator=True\n",
    "        )\n",
    "        error_metrics_table['scaled']['Mean Abs Error'] = abs(resultados_escalonados['test_neg_mean_absolute_error'].mean())\n",
    "        error_metrics_table['scaled']['Mean Sqr Error'] = abs(resultados_escalonados['test_neg_mean_squared_error'].mean())\n",
    "        error_metrics_table['scaled']['Mean Sqr Log Error'] = abs(resultados_escalonados['test_neg_mean_squared_log_error'].mean())\n",
    "        error_metrics_table['scaled']['Mean Median Error'] = abs(resultados_escalonados['test_neg_median_absolute_error'].mean())\n",
    "        error_metrics_table['scaled']['R2 Score'] = abs(resultados_escalonados['test_r2'].mean())\n",
    "\n",
    "        if display_prediction:\n",
    "            k_current = 0\n",
    "            for estimator in resultados_escalonados['estimator']:\n",
    "                ppc_predict = estimator.predict(self.__scale(previsores_content))\n",
    "                predict_table = pd.DataFrame(index=[i for i in range(len(ppc_predict))],columns=['PPC correct', 'PPC predict', 'MAE'])\n",
    "\n",
    "                for i in range(len(ppc_predict)):\n",
    "                    predict_table['PPC correct'][i] = ppc[i]\n",
    "                    predict_table['PPC predict'][i] = ppc_predict[i]\n",
    "                    predict_table['MAE'][i] = abs(ppc[i] - ppc_predict[i])\n",
    "                    #print(ppc_predict[i], ppc[i])\n",
    "                print('k:', k_current)\n",
    "                display(predict_table)\n",
    "                k_current += 1\n",
    "\n",
    "        if display_feature_importance:\n",
    "            k_current = 0\n",
    "            columns = [i for i in range(self.__k)]\n",
    "            columns.append('Mean')\n",
    "            fi_scaled_table = pd.DataFrame(columns=columns, index=self.__dataset[metrics].columns)\n",
    "\n",
    "            for estimator in resultados_escalonados['estimator']:\n",
    "                fi_scaled_table[k_current] = feature_importance_of(\n",
    "                    estimator, escalonador(previsores_content), ppc, self.__dataset[metrics].columns\n",
    "                )\n",
    "                k_current += 1\n",
    "\n",
    "            for i in range(fi_scaled_table.shape[0]):\n",
    "                fi_scaled_table['Mean'][i] = fi_scaled_table.iloc[i,:-1].mean()\n",
    "\n",
    "\n",
    "            fi_final = pd.DataFrame(columns=['Metrics', 'Importance'])\n",
    "            fi_final['Metrics'] = fi_scaled_table.index.values\n",
    "            fi_final['Importance'] = fi_scaled_table['Mean'].values\n",
    "\n",
    "            fi_final = fi_final.sort_values(ascending=False, by='Importance')\n",
    "            display(fi_final)\n",
    "            plt.figure(figsize=(12,8))\n",
    "            plt.title(\"Feature importance - K = \" + str(self.__k) + \" - Mean\")\n",
    "            plt.axis([0, fi_final['Importance'].values.max(), 0, len(fi_final.index.values)])\n",
    "            sns.barplot(y=fi_final['Metrics'].values, x=fi_final['Importance'].values, orient='h')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        return error_metrics_table\n",
    "    \n",
    "    def feature_importance(self, metrics):\n",
    "        ppc = self.__dataset['PrimePathCoverage'].values\n",
    "        importance_dataset = pd.DataFrame(\n",
    "                [0]*len(metrics),\n",
    "                index=metrics,\n",
    "                columns=['importance']\n",
    "        )\n",
    "        previsores_content = d[metrics].values\n",
    "        resultados = cross_validate(\n",
    "                self.__regressor, \n",
    "                previsores_content, \n",
    "                ppc, \n",
    "                cv=self.__k, \n",
    "                scoring=self.__error_metrics,\n",
    "                return_estimator=True\n",
    "        )\n",
    "        resultados_escalonados = cross_validate(\n",
    "                self.__regressor, \n",
    "                escalonador(previsores_content), \n",
    "                ppc, \n",
    "                cv=self.__k, \n",
    "                scoring=self.__error_metrics,\n",
    "                return_estimator=True\n",
    "        )\n",
    "\n",
    "        print('-----< SEM ESCALONAMENTO >-----')\n",
    "        #print(resultados)\n",
    "        for idx,estimator in enumerate(resultados['estimator']):\n",
    "            feature_importances = pd.DataFrame(estimator.feature_importances_,\n",
    "                                               index=metrics,\n",
    "                                               columns=['importance'])\n",
    "            importance_dataset += feature_importances\n",
    "\n",
    "        #print(importance_dataset) \n",
    "        importance_dataset['mean'] = importance_dataset['importance'] / self.__k\n",
    "        plt.figure(figsize=(10,9))\n",
    "        plt.title(\"Feature importance - Random forest - K = 10 - Mean\")\n",
    "        plt.barh(metrics, importance_dataset['mean'].values)\n",
    "        plt.show()\n",
    "\n",
    "        importance_dataset = pd.DataFrame(\n",
    "                [0]*len(metrics),\n",
    "                index=metrics,\n",
    "                columns=['importance']\n",
    "        )\n",
    "        print('\\n')\n",
    "\n",
    "        print('-----< COM ESCALONAMENTO >-----')\n",
    "        for idx,estimator in enumerate(resultados_escalonados['estimator']):\n",
    "            feature_importances = pd.DataFrame(estimator.feature_importances_,\n",
    "                                               index=metrics,\n",
    "                                               columns=['importance'])\n",
    "            importance_dataset += feature_importances\n",
    "\n",
    "        importance_dataset['mean'] = importance_dataset['importance'] / self.__k\n",
    "        print(importance_dataset['mean'].sort_values(ascending=False))\n",
    "        plt.figure(figsize=(10,9))\n",
    "        plt.title(\"Feature importance - Random forest - K = 10 - Mean\")\n",
    "        plt.barh(metrics, importance_dataset['mean'].values)\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "208aed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlPpcEvaluator(ABC):\n",
    "    \n",
    "    def __init__(self, dataset, tot_seeds=0, auto_display=True):\n",
    "        self.error_noscaled_metrics_table = None\n",
    "        self.error_scaled_metrics_table = None\n",
    "        self.__dataset = dataset\n",
    "        self.__tot_seeds = tot_seeds\n",
    "        self.__viewer = MlPpcViewer(self)\n",
    "        self.__auto_display = auto_display\n",
    "    \n",
    "    def evaluate(self, metrics):\n",
    "        self.__build_noscaled_dataframe()\n",
    "        self.__build_scaled_dataframe()\n",
    "        self.evaluate_metrics(metrics)\n",
    "        \n",
    "        if self.__auto_display:\n",
    "            self.display_results()\n",
    "    \n",
    "    def __build_noscaled_dataframe(self):\n",
    "        self.error_noscaled_metrics_table = self.__build_dataframe('Without scaling')\n",
    "        \n",
    "    def __build_scaled_dataframe(self):\n",
    "        self.error_scaled_metrics_table = self.__build_dataframe('With scaling')\n",
    "        \n",
    "    def __build_dataframe(self, caption):\n",
    "        seeds = [i for i in range(self.__tot_seeds+1)]\n",
    "        \n",
    "        if self.__tot_seeds > 1:\n",
    "            seeds.append('Mean')\n",
    "        \n",
    "        dataframe = pd.DataFrame(\n",
    "            index=['Mean Abs Error', 'Mean Sqr Error', 'Mean Sqr Log Error', 'Mean Median Error', 'R2 Score'],\n",
    "            columns=seeds\n",
    "        )\n",
    "        dataframe.columns.name = 'Seed'\n",
    "        dataframe.index.name = 'Error Metrics'\n",
    "        \n",
    "        return dataframe\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate_metrics(self, metrics):\n",
    "        pass\n",
    "        \n",
    "    def display_results(self):\n",
    "        self.__viewer.display_scaled_evaluation()\n",
    "        self.__viewer.display_noscaled_evaluation()\n",
    "        \n",
    "    def get_noscaled_metrics_table(self):\n",
    "        return self.error_noscaled_metrics_table\n",
    "    \n",
    "    def get_scaled_metrics_table(self):\n",
    "        return self.error_scaled_metrics_table\n",
    "    \n",
    "    def get_total_seeds(self):\n",
    "        return self.__tot_seeds\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return self.__dataset\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.__dataset.get_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037b7e6",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        super(LinearRegressionMlPpcEvaluator, self).__init__(dataset, auto_display=False)\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        only_ec = self.__is_only_ec(metrics)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            self.__evaluate_metric(metric, only_ec)\n",
    "            self.display_results(metric)\n",
    "            \n",
    "    def __is_only_ec(self, metrics):\n",
    "        return (len(metrics) == 1) and (metrics[0] == 'EdgeCoverage')\n",
    "    \n",
    "    def __evaluate_metric(self, metric, only_ec=False):\n",
    "        evaluator = PpcEvaluator(self.__get_regressor(only_ec), self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate([metric])\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def __get_regressor(self, only_ec):\n",
    "        return LinearRegression(fit_intercept=False) if only_ec else LinearRegression(positive=True)\n",
    "    \n",
    "    def display_results(self, metric):\n",
    "        print('Metric:', metric, end='')\n",
    "        super().display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d131991",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf07f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvrMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        super(SvrMlPpcEvaluator, self).__init__(dataset)\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        evaluator = PpcEvaluator(self.__get_regressor(), self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate(metrics)\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def __get_regressor(self):\n",
    "        SVR(epsilon=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df876a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e51d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset, tot_seeds=0):\n",
    "        super(RandomForestMlPpcEvaluator, self).__init__(dataset, tot_seeds)\n",
    "\n",
    "    def evaluate_metrics(self, metrics):\n",
    "        total_seeds = self.get_total_seeds()\n",
    "        \n",
    "        for i in range(0, total_seeds + 1):\n",
    "            evaluator = PpcEvaluator(self.__get_regressor_using_seed(i), self.get_dataset())\n",
    "            error_metrics = evaluator.evaluate(metrics)\n",
    "            self.error_noscaled_metrics_table[i] = error_metrics['no_scaled']\n",
    "            self.error_scaled_metrics_table[i] = error_metrics['scaled']\n",
    "            \n",
    "        if total_seeds > 1:\n",
    "            self.__compute_mean_error()\n",
    "            \n",
    "    def __get_regressor_using_seed(self, seed):\n",
    "        return RandomForestRegressor(random_state=seed)\n",
    "    \n",
    "    def __compute_mean_error(self):\n",
    "        self.__compute_mean_error_of_dataframe(self.error_noscaled_metrics_table)\n",
    "        self.__compute_mean_error_of_dataframe(self.error_scaled_metrics_table)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __compute_mean_error_of_dataframe(dataframe):\n",
    "        dataframe['Mean']['Mean Abs Error'] = dataframe.iloc[0,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Error'] = dataframe.iloc[1,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Log Error'] = dataframe.iloc[2,:-1].mean()\n",
    "        dataframe['Mean']['Mean Median Error'] = dataframe.iloc[3,:-1].mean()\n",
    "        dataframe['Mean']['R2 Score'] = dataframe.iloc[4,:-1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestFeatureImportance():\n",
    "    \n",
    "    def __init__(self, dataset, k=10):\n",
    "        self.__k = k\n",
    "        self.__dataset = dataset.get_dataframe()\n",
    "        self.__ppc = self.__dataset['PrimePathCoverage'].values\n",
    "        self.__independent_variables = None\n",
    "        self.__error_metrics = [\n",
    "            'r2', 'max_error', 'neg_mean_absolute_error',\n",
    "            'neg_mean_squared_error', 'neg_root_mean_squared_error',\n",
    "            'neg_mean_squared_log_error', 'neg_median_absolute_error'\n",
    "        ]\n",
    "    \n",
    "    def evaluate(self, metrics):\n",
    "        self.__extract_independent_variables_using_metrics(metrics)\n",
    "        self.__evaluate_feature_importance_with_scaling(metrics)\n",
    "        self.__evaluate_feature_importance_without_scaling(metrics)\n",
    "        \n",
    "    def __extract_independent_variables_using_metrics(self, metrics):\n",
    "        self.__independent_variables = self.__dataset[metrics].values\n",
    "        \n",
    "    def __evaluate_feature_importance_without_scaling(self, metrics):\n",
    "        importance_dataset = self.__compute_feature_importance_without_scaling(metrics)\n",
    "        self.__display_feature_importance_table(importance_dataset, 'Without scaling')\n",
    "        self.__display_feature_importance_chart(importance_dataset, 'Without scaling', metrics)\n",
    "        \n",
    "    def __compute_feature_importance_without_scaling(self, metrics):\n",
    "        return self.__compute_feature_importance_using_independent_variables(\n",
    "            metrics, \n",
    "            self.__independent_variables\n",
    "        )\n",
    "    \n",
    "    def __compute_feature_importance_using_independent_variables(self, metrics, independent_variables):\n",
    "        importance_dataset = pd.DataFrame(\n",
    "                [0]*len(metrics),\n",
    "                index=metrics,\n",
    "                columns=['importance']\n",
    "        )\n",
    "\n",
    "        resultados = cross_validate(\n",
    "                RandomForestRegressor(), \n",
    "                independent_variables, \n",
    "                self.__ppc, \n",
    "                cv=self.__k, \n",
    "                scoring=self.__error_metrics,\n",
    "                return_estimator=True\n",
    "        )\n",
    "\n",
    "        for idx,estimator in enumerate(resultados['estimator']):\n",
    "            feature_importances = pd.DataFrame(estimator.feature_importances_,\n",
    "                                               index=metrics,\n",
    "                                               columns=['importance'])\n",
    "            importance_dataset += feature_importances\n",
    "\n",
    "        importance_dataset['mean'] = importance_dataset['importance'] / self.__k\n",
    "        \n",
    "        return importance_dataset\n",
    "    \n",
    "    def __display_feature_importance_table(self, importance_dataset, title):\n",
    "        feature_importance_series = importance_dataset['mean'].sort_values(ascending=False)\n",
    "        d = pd.DataFrame(columns=['Metrics', 'Feature importance'])\n",
    "        d['Metrics'] = feature_importance_series.index\n",
    "        d['Feature importance'] = feature_importance_series.values\n",
    "        self.__display_dataframe_using_title(d, title)\n",
    "        \n",
    "    def __display_dataframe_using_title(self, dataframe, title):\n",
    "        styled_dataframe = dataframe.style.set_caption(title).set_table_styles([{\n",
    "            'selector': 'caption',\n",
    "            'props': [\n",
    "                ('color', 'black'),\n",
    "                ('font-size', '16px')\n",
    "            ]\n",
    "        }])\n",
    "        display(styled_dataframe)\n",
    "        \n",
    "    def __display_feature_importance_chart(self, importance_dataset, title, metrics):\n",
    "        plt.figure(figsize=(10,9))\n",
    "        plt.title(\"Feature importance - Random forest - K = \" + str(self.__k) + \" - \" + title)\n",
    "        plt.barh(metrics, importance_dataset['mean'].values)\n",
    "        plt.show()\n",
    "        \n",
    "    def __evaluate_feature_importance_with_scaling(self, metrics):\n",
    "        importance_dataset = self.__compute_feature_importance_with_scaling(metrics)\n",
    "        self.__display_feature_importance_table(importance_dataset, 'With scaling')\n",
    "        self.__display_feature_importance_chart(importance_dataset, 'With scaling', metrics)\n",
    "        \n",
    "    def __compute_feature_importance_with_scaling(self, metrics):\n",
    "        return self.__compute_feature_importance_using_independent_variables(\n",
    "            metrics, \n",
    "            self.__scale(self.__independent_variables)\n",
    "        )\n",
    "    \n",
    "    def __scale(self, data):\n",
    "        scaler = MinMaxScaler()\n",
    "        return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf247957",
   "metadata": {},
   "source": [
    "## K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bb3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighborsMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        super(KNeighborsMlPpcEvaluator, self).__init__(dataset, tot_seeds)\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        evaluator = PpcEvaluator(self.__get_regressor(), self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate(metrics)\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def __get_regressor(self):\n",
    "        return KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686112e2",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset, tot_seeds=0):\n",
    "        super(NeuralNetworkMlPpcEvaluator, self).__init__(dataset, tot_seeds)\n",
    "\n",
    "    def evaluate_metrics(self, metrics):\n",
    "        total_seeds = self.get_total_seeds()\n",
    "        \n",
    "        for i in range(0, total_seeds + 1):\n",
    "            evaluator = PpcEvaluator(self.__get_regressor_using_seed(i), self.get_dataset())\n",
    "            error_metrics = evaluator.evaluate(metrics)\n",
    "            self.error_noscaled_metrics_table[i] = error_metrics['no_scaled']\n",
    "            self.error_scaled_metrics_table[i] = error_metrics['scaled']\n",
    "            \n",
    "        if total_seeds > 1:\n",
    "            self.__compute_mean_error()\n",
    "            \n",
    "    def __get_regressor_using_seed(self, seed):\n",
    "        return MLPRegressor(epsilon=0.8, random_state=seed)\n",
    "    \n",
    "    def __compute_mean_error(self):\n",
    "        self.__compute_mean_error_of_dataframe(self.error_noscaled_metrics_table)\n",
    "        self.__compute_mean_error_of_dataframe(self.error_scaled_metrics_table)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __compute_mean_error_of_dataframe(dataframe):\n",
    "        dataframe['Mean']['Mean Abs Error'] = dataframe.iloc[0,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Error'] = dataframe.iloc[1,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Log Error'] = dataframe.iloc[2,:-1].mean()\n",
    "        dataframe['Mean']['Mean Median Error'] = dataframe.iloc[3,:-1].mean()\n",
    "        dataframe['Mean']['R2 Score'] = dataframe.iloc[4,:-1].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
