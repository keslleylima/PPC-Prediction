{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PpcEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PpcEvaluator:\n",
    "    def __init__(self, regressor, dataset):\n",
    "        self.__regressor = (lambda: regressor)\n",
    "        self.__dataset = dataset.get_dataframe()\n",
    "        self.__scaler = MinMaxScaler()\n",
    "        self.__k = 10\n",
    "        self.__error_metrics = [\n",
    "            'r2', 'max_error', 'neg_mean_absolute_error',\n",
    "            'neg_mean_squared_error', 'neg_root_mean_squared_error',\n",
    "            'neg_mean_squared_log_error', 'neg_median_absolute_error'\n",
    "        ]\n",
    "        self.__error_metrics_table = None\n",
    "        \n",
    "    def set_scaler(self, scaler):\n",
    "        self.__scaler = scaler\n",
    "    \n",
    "    def __scale(self, dados):\n",
    "        return self.__scaler.fit_transform(dados)\n",
    "\n",
    "    def evaluate(self, metrics, only_scaled=False, display_prediction=False, display_feature_importance=False):\n",
    "        ppc = self.__dataset['PrimePathCoverage'].values\n",
    "        previsores_content = self.__dataset[metrics].values\n",
    "\n",
    "        self.__error_metrics_table = pd.DataFrame(\n",
    "            index=['Mean Abs Error', 'Mean Sqr Error', 'Mean Sqr Log Error', 'Mean Median Error', 'R2 Score'],\n",
    "            columns=['no_scaled', 'scaled']\n",
    "        )\n",
    "\n",
    "        if not only_scaled:\n",
    "            resultados = cross_validate(\n",
    "                    self.__regressor(), \n",
    "                    previsores_content, \n",
    "                    ppc, \n",
    "                    cv=self.__k, \n",
    "                    scoring=self.__error_metrics, \n",
    "                    return_estimator=True\n",
    "            )\n",
    "            self.__build_error_metrics_table(previsores_content, ppc, resultados, 'no_scaled')\n",
    "\n",
    "        resultados_escalonados = cross_validate(\n",
    "                self.__regressor(),\n",
    "                self.__scale(previsores_content), \n",
    "                ppc, \n",
    "                cv=self.__k, \n",
    "                scoring=self.__error_metrics, \n",
    "                return_estimator=True\n",
    "        )\n",
    "        \n",
    "        self.__build_error_metrics_table(self.__scale(previsores_content), ppc, resultados_escalonados, 'scaled')\n",
    "        \n",
    "        if display_feature_importance:\n",
    "            k_current = 0\n",
    "            columns = [i for i in range(self.__k)]\n",
    "            columns.append('Mean')\n",
    "            fi_scaled_table = pd.DataFrame(columns=columns, index=self.__dataset[metrics].columns)\n",
    "\n",
    "            for estimator in resultados_escalonados['estimator']:\n",
    "                fi_scaled_table[k_current] = feature_importance_of(\n",
    "                    estimator, escalonador(previsores_content), ppc, self.__dataset[metrics].columns\n",
    "                )\n",
    "                k_current += 1\n",
    "\n",
    "            for i in range(fi_scaled_table.shape[0]):\n",
    "                fi_scaled_table['Mean'][i] = fi_scaled_table.iloc[i,:-1].mean()\n",
    "\n",
    "\n",
    "            fi_final = pd.DataFrame(columns=['Metrics', 'Importance'])\n",
    "            fi_final['Metrics'] = fi_scaled_table.index.values\n",
    "            fi_final['Importance'] = fi_scaled_table['Mean'].values\n",
    "\n",
    "            fi_final = fi_final.sort_values(ascending=False, by='Importance')\n",
    "            display(fi_final)\n",
    "            plt.figure(figsize=(12,8))\n",
    "            plt.title(\"Feature importance - K = \" + str(self.__k) + \" - Mean\")\n",
    "            plt.axis([0, fi_final['Importance'].values.max(), 0, len(fi_final.index.values)])\n",
    "            sns.barplot(y=fi_final['Metrics'].values, x=fi_final['Importance'].values, orient='h')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        return self.__error_metrics_table\n",
    "    \n",
    "    def __build_error_metrics_table(self, previsores_content, ppc, estimator, label):\n",
    "        idx_best_estimator = estimator['test_neg_root_mean_squared_error'].argmax()\n",
    "        self.__error_metrics_table[label]['Mean Abs Error'] = abs(estimator['test_neg_mean_absolute_error'][idx_best_estimator])\n",
    "        self.__error_metrics_table[label]['Mean Sqr Error'] = abs(estimator['test_neg_mean_squared_error'][idx_best_estimator]) \n",
    "        self.__error_metrics_table[label]['Mean Sqr Log Error'] = abs(estimator['test_neg_mean_squared_log_error'][idx_best_estimator])\n",
    "        self.__error_metrics_table[label]['Mean Median Error'] = abs(estimator['test_neg_median_absolute_error'][idx_best_estimator]) \n",
    "        self.__error_metrics_table[label]['R2 Score'] = abs(estimator['test_r2'][idx_best_estimator]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MlPpcEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlPpcEvaluator(ABC):\n",
    "    \n",
    "    def __init__(self, dataset, tot_seeds=0, auto_display=True):\n",
    "        self.error_noscaled_metrics_table = None\n",
    "        self.error_scaled_metrics_table = None\n",
    "        self.__full_error_metrics_table_scaled = None\n",
    "        self.__full_error_metrics_table_noscaled = None\n",
    "        self.__dataset = dataset\n",
    "        self.__tot_seeds = tot_seeds\n",
    "        self.__viewer = MlPpcViewer(self)\n",
    "        self.__auto_display = auto_display\n",
    "        self.__scaler = MinMaxScaler()\n",
    "        self.__k = 10\n",
    "        self.__precision = 3\n",
    "    \n",
    "    def evaluate(self, metrics):\n",
    "        self.__build_noscaled_dataframe()\n",
    "        self.__build_scaled_dataframe()\n",
    "        self.evaluate_metrics(metrics)\n",
    "        self.__build_predict_error_table(metrics, 'scaled')\n",
    "        self.__build_predict_error_table(metrics, 'no_scaled')\n",
    "        \n",
    "        if self.__auto_display:\n",
    "            self.display_results()\n",
    "            \n",
    "    def __scale(self, data):\n",
    "        return self.__scaler.fit_transform(data)\n",
    "    \n",
    "    def __build_noscaled_dataframe(self):\n",
    "        self.error_noscaled_metrics_table = self.__build_dataframe('Without scaling')\n",
    "        \n",
    "    def __build_scaled_dataframe(self):\n",
    "        self.error_scaled_metrics_table = self.__build_dataframe('With scaling')\n",
    "        \n",
    "    def __build_dataframe(self, caption):\n",
    "        seeds = [i for i in range(self.__tot_seeds+1)]\n",
    "        \n",
    "        if self.__tot_seeds > 1:\n",
    "            seeds.append('Mean')\n",
    "        \n",
    "        dataframe = pd.DataFrame(\n",
    "            index=['Mean Abs Error', 'Mean Sqr Error', 'Mean Sqr Log Error', 'Mean Median Error', 'R2 Score'],\n",
    "            columns=seeds\n",
    "        )\n",
    "        dataframe.columns.name = 'Seed'\n",
    "        dataframe.index.name = 'Error Metrics'\n",
    "        \n",
    "        return dataframe\n",
    "    \n",
    "    def __build_predict_error_table(self, metrics, label):\n",
    "        has_ec = 'EdgeCoverage' in metrics\n",
    "        best_mae = 1\n",
    "        predict_metrics = None\n",
    "        ppc_predict = None\n",
    "        ppc_correct = None\n",
    "        ec = []\n",
    "        df = self.get_dataframe()\n",
    "        all_metrics = metrics.copy()\n",
    "        all_metrics.append('PrimePathCoverage')\n",
    "        kf = KFold(n_splits=self.__k, shuffle=False)\n",
    "        model = self.get_regressor()\n",
    "        df.index = df.index * 10\n",
    "        len_train = 0\n",
    "        len_test = 0\n",
    "\n",
    "        for train_index, test_index in kf.split(df):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            \n",
    "            X_train = df[metrics].iloc[train_index]\n",
    "            X_test = df[metrics].iloc[test_index]\n",
    "            y_train = df['PrimePathCoverage'].iloc[train_index]\n",
    "            y_test = df['PrimePathCoverage'].iloc[test_index]\n",
    "            \n",
    "            if label == 'scale':\n",
    "                X_train = self.__scale(X_train)\n",
    "                X_test = self.__scale(X_test)\n",
    "                \n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_predict = model.predict(X_test)\n",
    "            mae = mean_absolute_error(y_test, y_predict)\n",
    "\n",
    "            \"\"\"\n",
    "            if ppc_predict is None:\n",
    "                predict_metrics = X_test\n",
    "                ppc_predict = y_predict\n",
    "                ppc_correct = y_test.values\n",
    "            else:\n",
    "                predict_metrics = pd.concat([X_test, predict_metrics])\n",
    "                ppc_predict = np.concatenate([y_predict, ppc_predict])\n",
    "                ppc_correct = np.concatenate([y_test.values, ppc_correct])\n",
    "            \"\"\"\n",
    "            \n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                predict_metrics = X_test\n",
    "                ppc_predict = y_predict\n",
    "                ppc_correct = y_test.values\n",
    "                len_train = train.shape[0]\n",
    "                len_test = test.shape[0]\n",
    "\n",
    "        cyclomatic = df.loc[predict_metrics.index.values].iloc[:, 8].values\n",
    "        \n",
    "        if has_ec:\n",
    "            ec = df.loc[predict_metrics.index.values].iloc[:, 12].values\n",
    "        \n",
    "        if has_ec:\n",
    "            predict_table = pd.DataFrame(index=[i for i in range(len(ppc_predict))],columns=['Cyclomatic', 'EC correct', 'PPC correct', 'PPC predict', 'Error'])\n",
    "        else:\n",
    "            predict_table = pd.DataFrame(index=[i for i in range(len(ppc_predict))],columns=['Cyclomatic', 'PPC correct', 'PPC predict', 'Error'])\n",
    "\n",
    "        for i in range(len(ppc_predict)):\n",
    "            current_ppc_correct = ppc_correct[i]\n",
    "            current_ppc_predict = MathUtils.truncate(ppc_predict[i], self.__precision)\n",
    "\n",
    "            predict_table.loc[i, 'PPC correct'] = current_ppc_correct\n",
    "            predict_table.loc[i, 'PPC predict'] = current_ppc_predict\n",
    "            predict_table.loc[i, 'Cyclomatic'] = cyclomatic[i]\n",
    "            predict_table.loc[i, 'Error'] = abs(current_ppc_correct - current_ppc_predict)\n",
    "\n",
    "            if has_ec:\n",
    "                predict_table.loc[i, 'EC correct'] = ec[i]\n",
    "\n",
    "        predict_table.sort_values(by='Error', ascending=False, inplace=True)\n",
    "        \n",
    "        #print('Train:', len_train)\n",
    "        #print('Test:', len_test)\n",
    "        \n",
    "        if label == 'scaled':\n",
    "            self.__full_error_metrics_table_scaled = predict_table.copy()\n",
    "        elif label == 'no_scaled':\n",
    "            self.__full_error_metrics_table_noscaled = predict_table.copy()\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate_metrics(self, metrics):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_regressor(self):\n",
    "        pass\n",
    "        \n",
    "    def display_results(self):\n",
    "        self.__viewer.display_scaled_evaluation()\n",
    "        self.__viewer.display_noscaled_evaluation()\n",
    "        \n",
    "    def get_noscaled_metrics_table(self):\n",
    "        return self.error_noscaled_metrics_table\n",
    "    \n",
    "    def get_scaled_metrics_table(self):\n",
    "        return self.error_scaled_metrics_table\n",
    "    \n",
    "    def get_total_seeds(self):\n",
    "        return self.__tot_seeds\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return self.__dataset\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.__dataset.get_dataframe()\n",
    "    \n",
    "    def get_predict_table_scaled(self):\n",
    "        return self.__full_error_metrics_table_scaled\n",
    "    \n",
    "    def get_predict_table_noscaled(self):\n",
    "        return self.__full_error_metrics_table_noscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        super(LinearRegressionMlPpcEvaluator, self).__init__(dataset, auto_display=False)\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        only_ec = self.__is_only_ec(metrics)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            self.__evaluate_metric(metric, only_ec)\n",
    "            self.display_results(metric)\n",
    "            \n",
    "    def __is_only_ec(self, metrics):\n",
    "        return (len(metrics) == 1) and (metrics[0] == 'EdgeCoverage')\n",
    "    \n",
    "    def __evaluate_metric(self, metric, only_ec=False):\n",
    "        evaluator = PpcEvaluator(self.__get_regressor(only_ec), self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate([metric])\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def __get_regressor(self, only_ec):\n",
    "        return LinearRegression(fit_intercept=False) if only_ec else LinearRegression(positive=True)\n",
    "    \n",
    "    def get_regressor(self):\n",
    "        return self.__get_regressor(false)\n",
    "    \n",
    "    def display_results(self, metric):\n",
    "        print('Metric:', metric, end='')\n",
    "        super().display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvrMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        super(SvrMlPpcEvaluator, self).__init__(dataset)\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        evaluator = PpcEvaluator(self.get_regressor(), self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate(metrics)\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def get_regressor(self):\n",
    "        return SVR(epsilon=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset, tot_seeds=0, auto_display=True):\n",
    "        super(RandomForestMlPpcEvaluator, self).__init__(dataset, tot_seeds, auto_display=auto_display)\n",
    "\n",
    "    def evaluate_metrics(self, metrics):\n",
    "        total_seeds = self.get_total_seeds()\n",
    "        \n",
    "        for i in range(0, total_seeds + 1):\n",
    "            evaluator = PpcEvaluator(self.__get_regressor_using_seed(i), self.get_dataset())\n",
    "            error_metrics = evaluator.evaluate(metrics)\n",
    "            self.error_noscaled_metrics_table[i] = error_metrics['no_scaled']\n",
    "            self.error_scaled_metrics_table[i] = error_metrics['scaled']\n",
    "            \n",
    "        if total_seeds > 1:\n",
    "            self.__compute_mean_error()\n",
    "            \n",
    "    def __get_regressor_using_seed(self, seed):\n",
    "        return RandomForestRegressor(random_state=seed)\n",
    "    \n",
    "    def get_regressor(self):\n",
    "        return self.__get_regressor_using_seed(0)\n",
    "    \n",
    "    def __compute_mean_error(self):\n",
    "        self.__compute_mean_error_of_dataframe(self.error_noscaled_metrics_table)\n",
    "        self.__compute_mean_error_of_dataframe(self.error_scaled_metrics_table)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __compute_mean_error_of_dataframe(dataframe):\n",
    "        dataframe['Mean']['Mean Abs Error'] = dataframe.iloc[0,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Error'] = dataframe.iloc[1,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Log Error'] = dataframe.iloc[2,:-1].mean()\n",
    "        dataframe['Mean']['Mean Median Error'] = dataframe.iloc[3,:-1].mean()\n",
    "        dataframe['Mean']['R2 Score'] = dataframe.iloc[4,:-1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighborsMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset, auto_display=True):\n",
    "        super(KNeighborsMlPpcEvaluator, self).__init__(dataset, 0, auto_display=auto_display)\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        evaluator = PpcEvaluator(self.get_regressor(), self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate(metrics)\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def get_regressor(self):\n",
    "        return KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkMlPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset, tot_seeds=0, epsilon=0.5):\n",
    "        super(NeuralNetworkMlPpcEvaluator, self).__init__(dataset, tot_seeds)\n",
    "        self.__epsilon=epsilon\n",
    "\n",
    "    def evaluate_metrics(self, metrics):\n",
    "        total_seeds = self.get_total_seeds()\n",
    "        \n",
    "        for i in range(0, total_seeds + 1):\n",
    "            evaluator = PpcEvaluator(self.__get_regressor_using_seed(i), self.get_dataset())\n",
    "            error_metrics = evaluator.evaluate(metrics)\n",
    "            self.error_noscaled_metrics_table[i] = error_metrics['no_scaled']\n",
    "            self.error_scaled_metrics_table[i] = error_metrics['scaled']\n",
    "            \n",
    "        if total_seeds > 1:\n",
    "            self.__compute_mean_error()\n",
    "            \n",
    "    def __get_regressor_using_seed(self, seed):\n",
    "        return MLPRegressor(epsilon=self.__epsilon, activation='logistic', random_state=seed)\n",
    "    \n",
    "    def get_regressor(self):\n",
    "        return self.__get_regressor_using_seed(0)\n",
    "    \n",
    "    def __compute_mean_error(self):\n",
    "        self.__compute_mean_error_of_dataframe(self.error_noscaled_metrics_table)\n",
    "        self.__compute_mean_error_of_dataframe(self.error_scaled_metrics_table)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __compute_mean_error_of_dataframe(dataframe):\n",
    "        dataframe['Mean']['Mean Abs Error'] = dataframe.iloc[0,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Error'] = dataframe.iloc[1,:-1].mean()\n",
    "        dataframe['Mean']['Mean Sqr Log Error'] = dataframe.iloc[2,:-1].mean()\n",
    "        dataframe['Mean']['Mean Median Error'] = dataframe.iloc[3,:-1].mean()\n",
    "        dataframe['Mean']['R2 Score'] = dataframe.iloc[4,:-1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPpcEvaluator(MlPpcEvaluator):\n",
    "    \n",
    "    def __init__(self, dataset, regressor):\n",
    "        super(CustomPpcEvaluator, self).__init__(dataset)\n",
    "        self.__regressor = regressor\n",
    "    \n",
    "    def evaluate_metrics(self, metrics):\n",
    "        evaluator = PpcEvaluator(self.__regressor, self.get_dataset())\n",
    "        error_metrics = evaluator.evaluate(metrics)\n",
    "        self.error_noscaled_metrics_table[0] = error_metrics['no_scaled']\n",
    "        self.error_scaled_metrics_table[0] = error_metrics['scaled']\n",
    "        \n",
    "    def get_regressor(self):\n",
    "        return self.__regressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
