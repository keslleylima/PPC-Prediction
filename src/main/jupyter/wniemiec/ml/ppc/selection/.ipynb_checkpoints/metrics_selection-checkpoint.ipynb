{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %run ../dataset/dataset.ipynb\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from itertools import chain, combinations\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricsSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsSelection:\n",
    "    \"\"\"\n",
    "    Responsible for evaluating which is the best subset of metrics, through an\n",
    "    error metric, for a given machine learning algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    #           Constructor\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __init__(self, regressor: MlPpcEvaluator, history=10):\n",
    "        \"\"\"\n",
    "        Evaluates which is the best subset of metrics, through an error metric, \n",
    "        for a given machine learning algorithm.\n",
    "        \n",
    "        :param      regressor: Machine learning algorithm\n",
    "        :param      history: Ranking size of best metric combinations \n",
    "        \"\"\"\n",
    "        self.__evaluator = evaluator\n",
    "        self.__history = history\n",
    "        self.__last_result = None\n",
    "        \n",
    "    def evaluate_using_brute_force(self, metrics) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        The algorithm will choose each combination and calculate score of each \n",
    "        combination then choose best combination based on R2 score. This \n",
    "        algorithm is called 'exhaustive feature selection' or 'brute force \n",
    "        features selection'.\n",
    "        \n",
    "        :param      metrics Metrics to be evaluated\n",
    "        \n",
    "        :return:    Table containing top 10 best metrics combination\n",
    "        \"\"\"\n",
    "        top10_best_metrics = self.__generate_empty_top10_best_metrics_table()\n",
    "        \n",
    "        metrics_powerset = self.__generate_powerset_of_set(metrics)\n",
    "        metrics_powerset = self.__remove_empty_subset_of(metrics_powerset)\n",
    "        \n",
    "        lowest_r2_of_top10_best_metrics = self.__get_worst_r2_error_metric_of(top10_best_metrics)\n",
    "        \n",
    "        for subset in metrics_powerset:\n",
    "            error_metrics = self.__evaluate(subset)\n",
    "            r2_error_metric_of_subset = self.__get_r2_error_metric_of(error_metrics)\n",
    "            \n",
    "            if r2_error_metric_of_subset > lowest_r2_of_top10_best_metrics:\n",
    "                self.__update_best_metrics_table(top10_best_metrics, r2_error_metric_of_subset, subset)\n",
    "                lowest_r2_of_top10_best_metrics = self.__get_worst_r2_error_metric_of(top10_best_metrics)\n",
    "\n",
    "        return top10_best_metrics \n",
    "    \n",
    "    def __generate_empty_top10_best_metrics_table(self):\n",
    "        return pd.DataFrame(\n",
    "            index=[i for i in range(self.__history)],\n",
    "            columns=['R2', 'Metrics'],\n",
    "            data=[[0.0, '']]\n",
    "        )\n",
    "\n",
    "    def __generate_powerset_of_set(self, iterable):\n",
    "        \"list(powerset([1,2,3])) --> [(), (1,), (2,), (3,), (1,2), (1,3), (2,3), (1,2,3)]\"\n",
    "        s = list(iterable)\n",
    "        it = chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "        powerset_list = []\n",
    "\n",
    "        for element in it:\n",
    "            subset_list = []\n",
    "            for subset_element in element:\n",
    "                subset_list.append(subset_element)\n",
    "\n",
    "            powerset_list.append(subset_list)\n",
    "\n",
    "        return powerset_list\n",
    "    \n",
    "    def __remove_empty_subset_of(self, s):\n",
    "        s.remove([])\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def __get_worst_r2_error_metric_of(self, dataframe):\n",
    "        idx_worst_r2 = dataframe['R2'].argmin()\n",
    "        \n",
    "        return dataframe.loc[idx_worst_r2, 'R2']\n",
    "    \n",
    "    def __evaluate(self, metrics):\n",
    "        self.__evaluator.evaluate(metrics)\n",
    "        return self.__evaluator.get_noscaled_metrics_table()\n",
    "    \n",
    "    def __get_r2_error_metric_of(self, dataframe):\n",
    "        return float(dataframe[0]['R2 Score'])\n",
    "    \n",
    "    def __update_best_metrics_table(self, dataframe, r2, metrics):\n",
    "        idx_worst_r2 = dataframe['R2'].argmin()\n",
    "        dataframe.loc[idx_worst_r2, 'R2'] = r2\n",
    "        dataframe.loc[idx_worst_r2, 'Metrics'] = self.__convert_set_to_text(metrics)\n",
    "        \n",
    "    def __convert_set_to_text(self, s):\n",
    "        return str([str(element) for element in s])\n",
    "    \n",
    "    def display_last_result(self):\n",
    "        pd.set_option('display.max_colwidth', 500)\n",
    "        results.sort_values(by=\"R2\", ascending=False)\n",
    "        display(self.__last_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
